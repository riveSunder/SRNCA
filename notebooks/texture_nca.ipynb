{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import skimage\n",
    "import skimage.io as sio\n",
    "import skimage.transform\n",
    "\n",
    "from srnca.nca import NCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.animation\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True).features\n",
    "\n",
    "identity = torch.tensor([[0., 0., 0.],\n",
    "                         [0., 1., 0.],\n",
    "                         [0., 0., 0.]])\n",
    "sobel_h = torch.tensor([[-1., -1., -1.],\n",
    "                        [0., 0., 0.],\n",
    "                        [1., 1., 1.]])\n",
    "sobel_w = torch.tensor([[-1., 0., 1.],\n",
    "                        [-1., 0., 1.],\n",
    "                        [-1., 0., 1.]])\n",
    "moore = torch.tensor([[1., 1., 1.],\n",
    "                      [1., 0., 1.] ,\n",
    "                      [1., 1., 1.]])\n",
    "laplacian = torch.tensor([[1., 2., 1.], \n",
    "                          [2., -12., 2], \n",
    "                          [1., 2., 1.]])\n",
    "def compute_grams(imgs):\n",
    "    \n",
    "    style_layers = [1, 6, 11, 18, 25]  \n",
    "    \n",
    "    # from https://github.com/google-research/self-organising-systems\n",
    "    # no idea why\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:,None,None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:,None,None]\n",
    "    x = (imgs-mean) / std\n",
    "    \n",
    "    grams = []\n",
    "    for i, layer in enumerate(vgg16[:max(style_layers)+1]):\n",
    "        x = layer(x)\n",
    "        if i in style_layers:\n",
    "            \n",
    "            h, w = x.shape[-2:]\n",
    "            y = x.clone()  # workaround for pytorch in-place modification bug(?)\n",
    "            \n",
    "            gram = torch.einsum('bchw, bdhw -> bcd', y, y) / (h*w)\n",
    "            grams.append(gram)\n",
    "            \n",
    "    return grams\n",
    "\n",
    "def compute_style_loss(grams_pred, grams_target):\n",
    "    \n",
    "    loss = 0.0\n",
    "    \n",
    "    for x, y in zip(grams_pred, grams_target):\n",
    "        loss = loss + (x-y).square().mean()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def read_image(url, max_size=None):\n",
    "    \n",
    "    img = sio.imread(url)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        img = skimage.transform.resize(img, (max_size, max_size))\n",
    "    \n",
    "   \n",
    "    img = np.float32(img)/ img.max()\n",
    "    \n",
    "    return img\n",
    "\n",
    "def image_to_tensor(img):\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        my_tensor = torch.tensor(img[np.newaxis, np.newaxis, ...])\n",
    "    elif len(img.shape) == 3:\n",
    "        my_tensor = torch.tensor(img.transpose(2,0,1)[np.newaxis,...])\n",
    "    \n",
    "    return my_tensor\n",
    "\n",
    "def tensor_to_image(my_tensor, index=0):\n",
    "\n",
    "    img = my_tensor[index,...].permute(1,2,0).detach().numpy()\n",
    "    \n",
    "    return img\n",
    "\n",
    "def perceive(x, filters):\n",
    "    \n",
    "    batch, channels, height, width = x.shape\n",
    "    \n",
    "    x = x.reshape(batch*channels, 1, height, width)\n",
    "    x = F.pad(x, (1,1,1,1), mode=\"circular\")\n",
    "    \n",
    "    x = F.conv2d(x, filters[:, np.newaxis, :, :])\n",
    "    \n",
    "    perception = x.reshape(batch, -1, height, width)\n",
    "    \n",
    "    return perception\n",
    "\n",
    "def plot_grid(grid):\n",
    "\n",
    "    global subplot_0\n",
    "            \n",
    "    fig, ax = plt.subplots(1,1, figsize=(7.5,7.5), facecolor=\"white\")\n",
    "\n",
    "    grid_display = tensor_to_image(grid)\n",
    "    \n",
    "    subplot_0 = ax.imshow(grid_display, interpolation=\"nearest\")\n",
    "   \n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xticklabels('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def update_fig(i):\n",
    "\n",
    "    global subplot_0    \n",
    "    global grid\n",
    "    \n",
    "    grid = nca(grid)\n",
    "    grid_display = tensor_to_image(grid)\n",
    "    \n",
    "    subplot_0.set_array(grid_display)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(my_seed=42):\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "    np.random.seed(my_seed)\n",
    "    \n",
    "def train(target, nca, max_steps, lr=1e-3, max_ca_steps=8):\n",
    "    \n",
    "    display_every = max_steps // 8 + 1\n",
    "    \n",
    "    optimizer = torch.optim.Adam(nca.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [max_steps//3], 0.3)\n",
    "    \n",
    "    grids = nca.get_init_grid(batch_size=64, dim=target.shape[-2])\n",
    "    \n",
    "    grams_target = compute_grams(target)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_index = np.random.choice(len(grids), 4, replace=False)\n",
    "            x = grids #[batch_index]\n",
    "\n",
    "            if step % 8 == 0:\n",
    "                x[:1] = nca.get_init_grid(batch_size=1, dim=x.shape[-2])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for ca_step in range(np.random.randint(1,12) + max_ca_steps):\n",
    "            x = nca(x)\n",
    "            \n",
    "        grams_pred  = compute_grams(x)\n",
    "        grams_target  = compute_grams(target)\n",
    "        \n",
    "        loss = compute_style_loss(grams_pred, grams_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        #grids[batch_index] = x\n",
    "    \n",
    "        if step % display_every == 0:\n",
    "            print(f\"loss at step {step} = {loss:.4e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_clamp = lambda x: 1.0 / (1.0 + torch.exp(-4.0 * (x-0.5)))                  \n",
    "\n",
    "class NCA(nn.Module):\n",
    "\n",
    "    def __init__(self, number_channels=1, number_filters=5, number_hidden=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.number_channels = number_channels\n",
    "        self.number_filters = number_filters\n",
    "        self.number_hidden = number_hidden\n",
    "\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(self.number_channels * self.number_filters, \\\n",
    "                self.number_hidden, kernel_size=1)\n",
    "        self.conv_1 = nn.Conv2d(self.number_hidden, self.number_channels, \\\n",
    "                kernel_size=1, bias=False)\n",
    "        self.filters = torch.stack([identity, sobel_h, sobel_w, \\\n",
    "                moore, laplacian])\n",
    "\n",
    "        self.conv_1.weight.data.zero_()\n",
    "\n",
    "        self.dt = 0.5 #nn.Parameter(torch.Tensor([[[[1.0]]]]))\n",
    "        #self.add_module(\"dt\", dt)\n",
    "        self.max_value = 1.0\n",
    "        self.min_value = 0.0\n",
    "\n",
    "        self.squash = soft_clamp\n",
    "\n",
    "\n",
    "    def forward(self, grid, update_rate=0.75):\n",
    "    \n",
    "\n",
    "        update_mask = (torch.rand_like(grid) < update_rate) * 1.0\n",
    "        perception = perceive(grid, self.filters)\n",
    "\n",
    "        new_grid = self.conv_0(perception)\n",
    "        new_grid = self.conv_1(new_grid)\n",
    "        \n",
    "        new_grid = grid + self.dt * new_grid * update_mask\n",
    "\n",
    "        return self.squash(new_grid)\n",
    "\n",
    "    def get_init_grid(self, batch_size=8, dim=128):\n",
    "        \n",
    "        temp = torch.zeros(batch_size, self.number_channels, dim, dim)\n",
    "\n",
    "        return temp\n",
    "\n",
    "    def initialize_optimizer(self, lr, max_steps):\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\\\n",
    "                self.optimizer, [max_steps // 3], 0.3)\n",
    "\n",
    "    def fit(self, target, max_steps=10, lr=1e-3, max_ca_steps=16, batch_size=8):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        display_every = max_steps // 8 + 1\n",
    "\n",
    "        self.initialize_optimizer(lr, max_steps)\n",
    "\n",
    "        grids = self.get_init_grid(batch_size=self.batch_size, dim = target.shape[-2])\n",
    "\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_index = np.random.choice(len(grids), 4, replace=False)\n",
    "\n",
    "                x = grids\n",
    "\n",
    "                if step % 8 == 0:\n",
    "                    x[:1] = self.get_init_grid(batch_size=1, dim=x.shape[-2])\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            for ca_step in range(np.random.randint(1,16) + max_ca_steps):\n",
    "                x = self.forward(x)\n",
    "            \n",
    "            grams_pred = compute_grams(x)\n",
    "            grams_target = compute_grams(target)# + torch.rand_like(target)*0.05)\n",
    "\n",
    "            loss = compute_style_loss(grams_pred, grams_target)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "            if step % display_every == 0:\n",
    "                print(f\"loss at step {step} = {loss:.4e}\")\n",
    "\n",
    "    def count_parameters(self):\n",
    "\n",
    "        number_parameters = 0\n",
    "\n",
    "        for param in self.parameters():\n",
    "            number_parameters += param.numel()\n",
    "\n",
    "        return number_parameters\n",
    "\n",
    "\n",
    "    def save_parameters(self, save_path):\n",
    "\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "\n",
    "    def load_parameters(self, load_path):\n",
    "\n",
    "        state_dict = torch.load(load_path)\n",
    "\n",
    "        self.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nasa.gov/centers/ames/images/content/72511main_cellstructure8.jpeg\"\n",
    "\n",
    "url = \"../data/images/orbia_magma.png\"\n",
    "\n",
    "#url = \"../data/images/jwst_segment_alignment.jpg\"\n",
    "\n",
    "img = read_image(url, max_size=128)[:,:,:3]\n",
    "\n",
    "\n",
    "target = image_to_tensor(img)\n",
    "img = tensor_to_image(target)\n",
    "print(target.shape)\n",
    "seed_all(13)\n",
    "\n",
    "nca = NCA(number_channels=3, number_hidden=256)\n",
    "\n",
    "# view the training image\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for textureso\n",
    "print(target.mean(), target.max(), target.min())\n",
    "nca.fit(target, max_steps=128, max_ca_steps=20, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 128\n",
    "\n",
    "grid = nca.get_init_grid(batch_size=1, dim=128)\n",
    "\n",
    "fig, ax = plot_grid(grid)\n",
    "\n",
    "plt.close(\"all\")\n",
    "IPython.display.HTML(matplotlib.animation.FuncAnimation(fig, update_fig, frames=num_frames, interval=100).to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
