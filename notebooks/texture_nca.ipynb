{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage\n",
    "import skimage.io as sio\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = torch.tensor([[0., 0., 0.],\n",
    "                         [0., 1., 0.],\n",
    "                         [0., 0., 0.]])\n",
    "sobel_h = torch.tensor([[-1., -1., -1.],\n",
    "                        [0., 0., 0.],\n",
    "                        [1., 1., 1.]])\n",
    "sobel_w = torch.tensor([[-1., 0., 1.],\n",
    "                        [-1., 0., 1.],\n",
    "                        [-1., 0., 1.]])\n",
    "moore = torch.tensor([[1., 1., 1.],\n",
    "                      [1., 0., 1.] ,\n",
    "                      [1., 1., 1.]])\n",
    "laplacian = torch.tensor([[1., 2., 1.], \n",
    "                          [2., -12., 2], \n",
    "                          [1., 2., 1.]])\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).features\n",
    "\n",
    "def compute_grams(imgs):\n",
    "    \n",
    "    style_layers = [1, 6, 11, 18, 25]  \n",
    "    \n",
    "    # from https://github.com/google-research/self-organising-systems\n",
    "    # no idea why\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:,None,None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:,None,None]\n",
    "    x = (imgs-mean) / std\n",
    "    \n",
    "    grams = []\n",
    "    for i, layer in enumerate(vgg16[:max(style_layers)+1]):\n",
    "        x = layer(x)\n",
    "        if i in style_layers:\n",
    "            \n",
    "            h, w = x.shape[-2:]\n",
    "            y = x.clone()  # workaround for pytorch in-place modification bug(?)\n",
    "            \n",
    "            gram = torch.einsum('bchw, bdhw -> bcd', y, y) / (h*w)\n",
    "            grams.append(gram)\n",
    "            \n",
    "    return grams\n",
    "\n",
    "def compute_style_loss(grams_pred, grams_target):\n",
    "    \n",
    "    loss = 0.0\n",
    "    \n",
    "    for x, y in zip(grams_pred, grams_target):\n",
    "        loss = loss + (x-y).square().mean()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def read_image(url, max_size=None):\n",
    "    \n",
    "    img = sio.imread(url)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        img = skimage.transform.resize(img, (max_size, max_size))\n",
    "    \n",
    "   \n",
    "    img = np.float32(img)/ img.max()\n",
    "    \n",
    "    return img\n",
    "\n",
    "def image_to_tensor(img):\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        my_tensor = torch.tensor(img[np.newaxis, np.newaxis, ...])\n",
    "    elif len(img.shape) == 3:\n",
    "        my_tensor = torch.tensor(img.transpose(2,0,1)[np.newaxis,...])\n",
    "    \n",
    "    return my_tensor\n",
    "\n",
    "def tensor_to_image(my_tensor, index=0):\n",
    "\n",
    "    img = my_tensor[index,...].permute(1,2,0).detach().numpy()\n",
    "    \n",
    "    return img\n",
    "\n",
    "def perceive(x, filters):\n",
    "    \n",
    "    batch, channels, height, width = x.shape\n",
    "    \n",
    "    x = x.reshape(batch*channels, 1, height, width)\n",
    "    x = F.pad(x, (1,1,1,1), mode=\"circular\")\n",
    "    \n",
    "    x = F.conv2d(x, filters[:, np.newaxis, :, :])\n",
    "    \n",
    "    perception = x.reshape(batch, -1, height, width)\n",
    "    \n",
    "    return perception\n",
    "    \n",
    "class NCA(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_channels=1, number_filters=5, number_hidden=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.number_channels = number_channels\n",
    "        self.conv_0 = nn.Conv2d(number_channels * number_filters, number_hidden, \\\n",
    "                kernel_size=1)\n",
    "        self.conv_1 = nn.Conv2d(number_hidden, number_channels, \\\n",
    "                kernel_size=1, bias=False)\n",
    "        \n",
    "        self.filters = torch.stack([identity, sobel_h, sobel_w, moore, laplacian])\n",
    "        \n",
    "        # Trick from Mordvintsev\n",
    "        self.conv_1.weight.data.zero_()\n",
    "        \n",
    "        # update step size\n",
    "        self.dt = 1.0\n",
    "        self.max_value = 1.0\n",
    "        self.min_value = 0.0\n",
    "    \n",
    "    def forward(self, grid, update_rate=0.5):\n",
    "        \n",
    "        update_mask = (torch.rand_like(grid) < update_rate) * 1.0\n",
    "        perception = perceive(grid, self.filters)\n",
    "        \n",
    "        \n",
    "        new_grid = self.conv_0(perception)\n",
    "        new_grid = self.conv_1(new_grid)\n",
    "        \n",
    "        return torch.clamp(grid + self.dt * new_grid * update_mask, \\\n",
    "                self.min_value, self.max_value)\n",
    "    \n",
    "    def get_init_grid(self, batch_size=8, dim=128):\n",
    "        \n",
    "        temp = torch.zeros(batch_size, self.number_channels, dim, dim)\n",
    "        \n",
    "        #temp[:,:,dim//2:dim//2+10, dim//2:dim//2+10] = self.max_value\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(my_seed=42):\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "    np.random.seed(my_seed)\n",
    "    \n",
    "def train(target, nca, max_steps, lr=1e-3, max_ca_steps=8):\n",
    "    \n",
    "    display_every = max_steps // 8 + 1\n",
    "    \n",
    "    optimizer = torch.optim.Adam(nca.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [max_steps//3], 0.3)\n",
    "    \n",
    "    grids = nca.get_init_grid(batch_size=64, dim=target.shape[-2])\n",
    "    \n",
    "    grams_target = compute_grams(target)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_index = np.random.choice(len(grids), 4, replace=False)\n",
    "            x = grids #[batch_index]\n",
    "\n",
    "            if step % 8 == 0:\n",
    "                x[:1] = nca.get_init_grid(batch_size=1, dim=x.shape[-2])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for ca_step in range(max_ca_steps):\n",
    "            x = nca(x)\n",
    "            \n",
    "        grams_pred  = compute_grams(x)\n",
    "        grams_target  = compute_grams(target)\n",
    "        \n",
    "        loss = compute_style_loss(grams_pred, grams_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        #grids[batch_index] = x\n",
    "    \n",
    "        if step % display_every == 0:\n",
    "            print(f\"loss at step {step} = {loss:.4e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nasa.gov/centers/ames/images/content/72511main_cellstructure8.jpeg\"\n",
    "\n",
    "img = read_image(url, max_size=128)\n",
    "target = image_to_tensor(img)\n",
    "\n",
    "seed_all(1337)\n",
    "\n",
    "nca = NCA(number_channels=3)\n",
    "\n",
    "# view the training image\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for textures\n",
    "train(target, nca, max_steps=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3602a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize what the nca model puts out\n",
    "\n",
    "grid = nca.get_init_grid(batch_size=1, dim=256)\n",
    "\n",
    "img = tensor_to_image(grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "for step in range(2):\n",
    "    grid = nca(grid)\n",
    "\n",
    "img = tensor_to_image(grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "for step in range(32):\n",
    "    grid = nca(grid)\n",
    "\n",
    "img = tensor_to_image(grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do the perception convolutions look like?\n",
    "filters = torch.stack([identity, sobel_h, sobel_w, moore, laplacian])\n",
    "perception = perceive(target, filters)\n",
    "\n",
    "#image/filter dimensions\n",
    "print(perception.shape, img.shape)\n",
    "\n",
    "#visualize perception \n",
    "for ii in range(perception.shape[1]):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(perception[0,ii,:,:].detach())\n",
    "    plt.title(f\"channel {ii}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
